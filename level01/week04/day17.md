# Day17 (2022/02/08)

## 노트

### Transformer

구조를 이해하는 것보다 어떻게 사용되는지를 이해하는 것이 더 중요한 것 같다. 작년 초에도 같은 생각이어서 아래의 논문들을 읽었지만 지금은 하나도 기억이 나지 않는다. 따라서 여기에 구조를 자세히 정리하는 대신 아래의 논문들을 나누어 읽기로 한다.

#### Original Proposal

> https://arxiv.org/pdf/1706.03762

  * Attention Is All You Need

#### Survey

> https://arxiv.org/pdf/2101.01169

  * Transformers in Vision: A Survey

#### Related Works

  * Non-local Neural Networks
  * BERT
  * ViT
  * UNITER
  * Unicoder
  * ViLBERT
  * Oscar
  * 12-in-1
  * VILLA
  * LXMERT
  * VirTex
  * DEIT
  * DETR
  * Deformable DETR
  * Generative Pretraining from Pixels

## 일지

### Daily scrum (10:00-10:10)

### 강의 영상 수강 및 과제 제출 (10:10-12:00)

  * [강의] Deep Learning Basics
    * RNN
    * LSTM Implementation
  * [과제] Deep Learning Basics
    * LSTM

### 보충 학습 (13:00-13:40)

> https://github.com/nestiank/cv-paper-study

  * 예전에 작성한 논문 정리 노트를 다시 정리

### 강의 영상 수강 및 퀴즈 제출 (13:40-15:40)

  * [강의] Deep Learning Basics
    * Transformer
    * SDPA & MHA Implementation
  * [퀴즈] Deep Learning Basics
    * RNN

### 과제 제출 (15:40-16:00)

  * [과제] Deep Learning Basics
    * MHA

### Peer session (16:00-17:00)

  * 커리어 관련 이야기
  * ResNet 논문 이야기
    * Gradient vanishing과 degradation의 연관성 이야기
  * 다음 peer session 준비
    * 다음 주 월요일까지 transformer proposal 논문 리뷰해 오기

### Daily report 작성 (17:00-19:00)

### Mentoring (19:00-20:10)

  * ConvNeXt 소개
  * Diffusion model 소개
  * SDEdit 소개
  * Adversarial attack 소개
  * 사전 질문들에 대한 답변
